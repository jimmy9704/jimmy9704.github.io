<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Hongjae Lee</title> <meta name="author" content="Hongjae Lee"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/%F0%9F%A7%91%F0%9F%8F%BB%E2%80%8D%F0%9F%8E%93"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jimmy9704.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://jimmy9704.github.io/"><span class="font-weight-bold">Hongjae</span> Lee</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/refqsr-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/refqsr-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/refqsr-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/refqsr.png" data-zoomable=""> </picture> </figure> </div> <div id="lee2024refqsr" class="col-sm-8"> <div class="title">RefQSR: Reference-based Quantization for Image Super-Resolution Networks</div> <div class="author"> <em>Lee, Hongjae</em>, Yoo, Jun-Sang, and Jung, Seung-Won </div> <div class="periodical"> <em>IEEE TIP</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.01690" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://jimmy9704.github.io/RefQSR/" class="btn btn-sm z-depth-0" role="button">PAGE</a> </div> <div class="abstract hidden"> <p>Single image super-resolution (SISR) aims to reconstruct a high-resolution image from its low-resolution observation. Recent deep learning-based SISR models show high performance at the expense of increased computational costs, limiting their use in resource-constrained environments. As a promising solution for computationally efficient network design, network quantization has been extensively studied. However, existing quantization methods developed for SISR have yet to effectively exploit image self-similarity, which is a new direction for exploration in this study. We introduce a novel method called reference-based quantization for image super-resolution (RefQSR) that applies high-bit quantization to several representative patches and uses them as references for low-bit quantization of the rest of the patches in an image. To this end, we design dedicated patch clustering and reference-based quantization modules and integrate them into existing SISR network quantization methods. The experimental results demonstrate the effectiveness of RefQSR on various SISR networks and quantization methods.</p> </div> </div> </div> </li></ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/gps_glass-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/gps_glass-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/gps_glass-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/gps_glass.png" data-zoomable=""> </picture> </figure> </div> <div id="lee2022gpsglass" class="col-sm-8"> <div class="title">GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS data</div> <div class="author"> <em>Lee, Hongjae</em>, Han, Changwoo, Yoo, Jun-Sang, and Jung, Seung-Won </div> <div class="periodical"> <em>ICCVW (oral)</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.13297" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/jimmy9704/GPS-GLASS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Semantic segmentation for autonomous driving should be robust against various in-the-wild environments. Nighttime semantic segmentation is especially challenging due to a lack of annotated nighttime images and a large domain gap from daytime images with sufficient annotation. In this paper, we propose a novel GPS-based training framework for nighttime semantic segmentation. Given GPS-aligned pairs of daytime and nighttime images, we perform cross-domain correspondence matching to obtain pixel-level pseudo supervision. Moreover, we conduct flow estimation between daytime video frames and apply GPS-based scaling to acquire another pixel-level pseudo supervision. Using these pseudo supervisions with a confidence map, we train a nighttime semantic segmentation network without any annotation from nighttime images. Experimental results demonstrate the effectiveness of the proposed method on several nighttime semantic segmentation datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/vos_vfi-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/vos_vfi-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/vos_vfi-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/vos_vfi.png" data-zoomable=""> </picture> </figure> </div> <div id="yoo2023vosvfi" class="col-sm-8"> <div class="title">Video Object Segmentation-aware Video Frame Interpolation</div> <div class="author">Yoo, Jun-Sang,  <em>Lee, Hongjae</em>, and Jung, Seung-Won </div> <div class="periodical"> <em>ICCV</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Video frame interpolation (VFI) is a very active research topic due to its broad applicability to many applications, including video enhancement, video encoding, and slow-motion effects. VFI methods have been advanced by improving the overall image quality for challenging sequences containing occlusions, large motion, and dynamic texture. This mainstream research direction neglects that foreground and background regions have different importance in perceptual image quality. Moreover, accurate synthesis of moving objects can be of utmost importance in computer vision applications. In this paper, we propose a video object segmentation (VOS)-aware training framework called VOS-VFI that allows VFI models to interpolate frames with more precise object boundaries. Specifically, we exploit VOS as an auxiliary task to help train VFI models by providing additional loss functions, including segmentation loss and bi-directional consistency loss. From extensive experiments, we demonstrate that VOS-VFI can boost the performance of existing VFI models by rendering clear object boundaries. Moreover, VOS-VFI displays its effectiveness on multiple benchmarks for different applications, including video object segmentation, object pose estimation, and visual tracking.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/hst-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/hst-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/hst-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/hst.png" data-zoomable=""> </picture> </figure> </div> <div id="yoo2022hst" class="col-sm-8"> <div class="title">Hierarchical Spatiotemporal Transformers for Video Object Segmentation</div> <div class="author">Yoo, Jun-Sang,  <em>Lee, Hongjae</em>, and Jung, Seung-Won </div> <div class="periodical"> <em>ICCVW (oral)</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.08263" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>This paper presents a novel framework called HST for semi-supervised video object segmentation (VOS). HST extracts image and video features using the latest Swin Transformer and Video Swin Transformer to inherit their inductive bias for the spatiotemporal locality, which is essential for temporally coherent VOS. To take full advantage of the image and video features, HST casts image and video features as a query and memory, respectively. By applying efficient memory read operations at multiple scales, HST produces hierarchical features for the precise reconstruction of object masks. HST shows effectiveness and robustness in handling challenging scenarios with occluded and fast-moving objects under cluttered backgrounds. In particular, HST-B outperforms the state-of-the-art competitors on multiple popular benchmarks.</p> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/phomonet-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/phomonet-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/phomonet-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/phomonet.png" data-zoomable=""> </picture> </figure> </div> <div id="lee2022phomonet" class="col-sm-8"> <div class="title">Monocular Depth Estimation Network with Single-Pixel Depth Guidance</div> <div class="author"> <em>Lee, Hongjae</em>, Park, Jinbum, Jeong, Wooseok, and Jung, Seung-Won </div> <div class="periodical"> <em>Optics Letters</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/jimmy9704/PhoMoNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Due to the scale ambiguity problem, the performance of monocular depth estimation (MDE) is inherently restricted. Multi-camera systems, especially those equipped with active depth cameras, have addressed this problem at the expense of increased hardware costs and space. In this Letter, we adopt a similar but costeffective solution using only single-pixel depth guidance with a single-photon avalanche diode. To this end, we design a single-pixel guidance module (SPGM) that combines the global information from the single-pixel depth guidance with the spatial information from the image at the feature level. By integrating SPGMs into an MDE network, we introduce PhoMoNet, the first end-to-end MDE network with single-pixel depth guidance. Experimental results show the effectiveness and superiority of PhoMoNet over state-of-the-art MDE networks on synthetic and real-world datasets.</p> </div> </div> </div> </li></ol> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/plane_segmentation-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/plane_segmentation-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/plane_segmentation-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/plane_segmentation.png" data-zoomable=""> </picture> </figure> </div> <div id="lee2021plane" class="col-sm-8"> <div class="title">Clustering-based plane segmentation neural network for urban scene modeling</div> <div class="author"> <em>Lee, Hongjae</em>, and Jung, Jiyoung </div> <div class="periodical"> <em>Sensors</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/jimmy9704/plane-segmentation-network" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Urban scene modeling is a challenging but essential task for various applications, such as 3D map generation, city digitization, and AR/VR/metaverse applications. To model man-made structures, such as roads and buildings, which are the major components in general urban scenes, we present a clustering-based plane segmentation neural network using 3D point clouds, called hybrid K-means plane segmentation (HKPS). The proposed method segments unorganized 3D point clouds into planes by training the neural network to estimate the appropriate number of planes in the point cloud based on hybrid K-means clustering. We consider both the Euclidean distance and cosine distance to cluster nearby points in the same direction for better plane segmentation results. Our network does not require any labeled information for training. We evaluated the proposed method using the Virtual KITTI dataset and showed that our method outperforms conventional methods in plane segmentation. Our code is publicly available.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom"> <div class="container mt-0" align="center"> <a href="https://info.flagcounter.com/nFZq" target="_blank" rel="noopener noreferrer"><img src="https://s01.flagcounter.com/count2/nFZq/bg_FFFFFF/txt_000000/border_CCCCCC/columns_3/maxflags_9/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>              <a href="https://info.flagcounter.com/Djm1" target="_blank" rel="noopener noreferrer"><img src="https://s11.flagcounter.com/map/Djm1/size_t/txt_000000/border_CCCCCC/pageviews_0/viewers_0/flags_0/" alt="Flag Counter" border="0"></a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>